# Advances and Open Problems in Federated Learning

## 0 什么是联邦学习

联邦学习是一类分布式机器学习，节点合作生成一个模型却不共享数据

那么，联邦学习与传统分布式机器学习区别是什么？

1. 用户拥有对于设备和数据的控制权
2. 用户节点是不稳定的
3. 通信代价高于计算代价
4. 数据在节点中是non-IID的
5. 数据量是非常不平衡的

由于联邦学习的这些限制，产生了三个主要的研究方向

1. 提高通信效率的算法
2. 抵御隐私泄露
3. 对于恶意节点具有健壮性（恶意节点可能是用户、服务器等等）

## 1 引言

联邦学习是一类机器学习，在中心服务器（如：服务提供商）的管理下多个客户端（如：移动设备或整个组织）共同训练一个模型，同时保持训练数据去中心化。联邦学习体现了集中数据收集和最小化的原理，并且可以减轻由传统的集中式机器学习和数据科学方法导致的许多系统隐私风险和成本。

联邦学习许多问题的关键特性是它们本质上是跨学科的，要解决这些问题可能不仅需要机器学习，还需要分布式优化，密码学，安全性，差分隐私，公平性，压缩感知，系统，信息论，统计等等。

所谓 跨域的联邦学习就是将联邦学习的某些限制放宽，比如通行效率、用户可靠性、用户节点的数据量等等，下面这张图是更详细的区分了这三者的关系

![表1](D:\study\GitHub\Federated-Learning-Notes\Advances and Open Problems in Federated Learning\2020-09-17汇报.assets\Table1.png)

表1：联邦学习环境与数据中心中分布式学习的典型特征

### 1.1 跨设备联邦学习环境

![](D:\study\GitHub\Federated-Learning-Notes\Advances and Open Problems in Federated Learning\2020-09-17汇报.assets\Figure1.png)

<center>图1：联邦学习训练的模型以及系统中各个参与者的生命周期</center>

#### 1.1.1 联邦学习中模型的生命周期

1. **问题辨别**：模型工程师判断问题是否要用联邦学习解决。
2. **客户端检测**：如果需要，可以对客户端（例如，在手机上运行的应用）进行检测，在本地存储必要的训练数据。
3. **仿真原型（可选）**：模型工程师可以使用代理数据集在联邦学习仿真中对模型体系结构进行原型设计并测试学习超参数。
4. **联邦学习模型训练**：开始执行多个联邦学习训练任务以训练模型的不同变体，或使用不同的优化超参数。
5. **（联邦）模型评估**：在对任务进行了充分的训练之后，将对模型进行分析并选择好的候选。
6. **部署**：最后，一旦选择好模型，它将经历一个标准模型启动过程，以及分阶段推出（以便在发现较差性能和影响太多用户之前回滚）。![](D:\study\GitHub\Federated-Learning-Notes\Advances and Open Problems in Federated Learning\2020-09-17汇报.assets\Table2.png)

<center>表2：典型的跨设备联合学习应用程序的数量级大小</center>

联邦学习系统面临的主要实际挑战之一是使上述工作流程尽可能简单，理想地接近机器学习系统实现集中训练的易用性。

#### 1.1.2 典型的联邦学习训练过程

1. **客户端选择**：服务器从满足资格要求的一组客户端中采样。 例如，移动电话可能仅在连接到未计费的wi-fi且处于空闲状态时才连入服务器，以避免影响设备用户。
2. **广播**：选定的客户端从服务器下载当前模型权重和训练程序。
3. **客户端计算**：每个选定的设备都通过执行训练程序在本地计算对模型的更新。
4. **聚合**：服务器收集设备更新的聚合。该阶段也是许多其他技术的集成点，稍后将讨论这些技术，其中可能包括：用于增加隐私的安全聚合，为了通信效率而对聚合进行有损压缩，以及针对差分隐私的噪声添加和更新限幅。
5. **模型更新**：服务器根据从参与当前轮次的客户端计算出的聚合更新，更新在服务器本地的共享模型。

### 1.2 跨域联邦学习

现实情境：许多公司或组织希望基于其他们的数据来训练模型，但不能直接共享其数据。

```
             +--------------------+
             | Federated Learning |
             +---------+----------+
                       |
          +------------+------------+
          v                         v
+---------+------------+   +--------+-----------+
| Fully Decentralized  |   | Cross-Silo         |
| / Peer-to-Peer       |   | Federated Learning |
| Distributed Learning |   +---------+----------+
| (Cross-Devices)      |             |
+----------------------+   +---------+----------+
                           v                    v
                    +------+-------+    +-------+------+
                    | partitioning |    | partitioning |
                    | by examples  |    | by features  |
                    +--------------+    +--------------+

```

**横向联邦学习**

在两个数据集的用户特征重叠较多而用户重叠较少的情况下，我们把数据集按照横向 (即用户维度)切分，并取出双方用户特征相同而用户不完全相同的那部分数据进行训练。这 种方法叫做横向联邦学习。比如有两家不同地区银行，它们的用户群体分别来自各自所在的地区，相互的交集很小。

**纵向联邦学习**

在两个数据集的用户重叠较多而用户特征重叠较少的情况下，我们把数据集按照纵向 （即特征维度）切分，并取出双方用户相同而用户特征不完全相同的那部分数据进行训练。 这种方法叫做纵向联邦学习。比如有两个不同机构，一家是某地的银行，另一家是同一个地方的电商。它们的用户群体很有可能包含该地的大部分居民，因此用户的交集较大。

**联邦迁移学习**

在两个数据集的用户与用户特征重叠都较少的情况下，我们不对数据进行切分，而可以 利用迁移学习来克服数据或标签不足的情况。这种方法叫做联邦迁移学习。 比如有两个不同机构，一家是位于中国的银行，另一家是位于美国的电商。由于受到地域限制，这两家机构的用户群体交集很小。同时，由于机构类型的不同，二者的数据特征也只有小部分重合。在这种情况下，要想进行有效的联邦学习，就必须引入迁移学习， 来解决单边数据规模小和标签样本少的问题，从而提升模型的效果。

![Figure1](D:\study\GitHub\Federated-Learning-Notes\Advances and Open Problems in Federated Learning\2020-09-17汇报.assets\Figure1-1600343698192.png)



## 2 提高效率和有效性

开发更好的优化算法； 为不同的客户提供不同的模型； 在联邦学习上下文中使诸如超参数搜索，架构搜索和调试之类的； 提高沟通效率；等等。

### 2.1 Non-IID Data in Federated Learning

1.不相同的客户分布

- 特征分布不平衡

- 标签分布不平衡

- 标签不同，特征相同

- 特征不同，标签相同

- 数量偏分布不平衡

non-IID数据集的大多数工作都集中在标签分布不平衡上。

2.数据不独立

3.数据集移位

#### 2.1.1 Strategies for Dealing with Non-IID Data

一种自然的方法是修改现有算法（例如，通过选择不同的超参数）

对于某些应用程序，可能有可能扩充数据，以使跨客户端的数据更加相似。

不再将所有的用户数据给与相同的权重，替代方法包括限制来自任何一个用户的数据贡献。

除了解决不相同的客户分布之外，使用多种模型还可以解决由于客户可用性变化而引起的对独立性的损害。

### 2.2 联邦学习算法优化

在典型的联合学习任务中，目标是要学习一个单一的全局模型，该模型可以使整个训练数据集上的经验风险函数最小化，以实现优化，non-IID和不平衡数据，有限的通信带宽以及不可靠和有限的设备可用性尤其重要。
设备总数巨大的联邦学习环境（例如，跨移动设备）需要每轮只需要几个客户端参与的算法（客户端采样）。 此外，每个设备可能最多只参与一次给定模型的训练，因此无状态算法是必需的。 这排除了直接在数据中心环境中非常有效的各种方法的直接应用。
与其他技术的可组合性。例如密码安全聚合协议，差分隐私以及模型和更新压缩。

![](D:\study\GitHub\Federated-Learning-Notes\Advances and Open Problems in Federated Learning\2020-09-17汇报.assets\Table4.png)

联邦平均算法

#### 2.2.1 超参数调整

在机器学习中超参数调整主要涉及如何提高模型的准确性，而不是针对移动设备的通信和计算效率。 

#### 2.2.2 神经网络设计

神经网络结构搜索（NAS）NAS有三种主要方法，它们利用进化算法，强化学习或梯度下降来搜索特定数据集上特定任务的最佳体系结构。

#### 2.2.3 联邦学习的查错和解释

对于一个联邦学习模型，训练是在本地进行的，如果模型出现问题，查找问题的根源将极为困难

#### 2.2.4 压缩

- 梯度压缩 – 减小了从客户端到服务器通信的对象的大小，该对象用于更新全局模型
- 模型广播压缩– 减小从服务器向客户端广播的模型的大小，客户端从该模型开始本地训练。
- 缩减本地计算 – 对整体训练算法的任何修改，以使本地训练过程在计算上更加有效。

## 3 保护用户数据隐私

我们提倡一种策略，其中整个系统由可以相对独立地研究和改进的模块化单元组成 。

### 3.1 相关技术

差分隐私 

Secure Multi-Party Computation (MPC) protocol 安全多方计算

Trusted Execution Environment (TEE) 可信执行平台

Private Disclosure techniques 隐私披露技术

remote attestation and zero-knowledge proofs 远程证明和零知识证明

以上的多种技术可用于提供可验证性：零知识证明（ZKP），可信执行环境（TEE）或远程证明。

（零知识证明 指的是证明者能够在不向验证者提供任何有用的信息的情况下，使验证者相信某个论断是正确的。）

![image-20200907090416666](D:\study\GitHub\Federated-Learning-Notes\Advances and Open Problems in Federated Learning\2020-09-17汇报.assets\image-20200907090416666.png)

### 3.2 潜在威胁

![](D:\study\GitHub\Federated-Learning-Notes\Advances and Open Problems in Federated Learning\2020-09-17汇报.assets\Table7.png)



#### 3.2.1 应对外部恶意攻击者的威胁

审核迭代过程和最终模型

集中差异隐私训练

隐藏迭代过程

对不断发展的数据进行重复分析

防止模型盗用和滥用

#### 3.2.2 内部威胁

以下只是内部威胁的一部分。

在跨设备联邦学习环境中，我们拥有一台具有大量计算资源的服务器和大量客户端，这些客户端（i）仅能与该服务器通信（如星型网络拓扑），并且（ii）连通性可能受到限制和带宽。
控制服务器的主动恶意攻击者可能会模拟大量伪造的客户端设备（“ Sybil攻击”  ），或者可以从可用设备池中优先选择以前受到破坏的设备。

#### 3.2.3 现有解决方案的局限性

**Local differential privacy**

LDP假定用户的隐私完全来自该用户自己的随机性； 因此，用户的隐私保证独立于所有其他用户添加的其他随机性。
造成这种困难的部分原因是，引入的随机噪声的幅度必须与数据中信号的幅度相当，这可能需要合并客户端之间的报告。
因此，要获得与中心差分隐私相同的效果，就需要相对较大的用户群或较大的参数选择。

**Hybrid differential privacy**

它不提供用户本地噪声的隐私放大功能。
目前尚不清楚哪个应用程序领域和算法可以最有效地利用混合信任模型数据，目前在混合模型上的工作通常假设无论用户信任偏好如何，其数据都来自相同的分布

**The shuffle model**

首先是可信中间层的要求

第二个缺点是，The shuffle model的差分隐私保证与参与计算的对手用户数量成比例地降低

**Secure aggregation**

这种方法有几个局限性：（a）假设一个半诚实的服务器（仅在私钥基础结构阶段），（b）允许服务器查看所有的聚合数据（可能仍会泄漏信息），（c  ）对于稀疏向量聚合而言效率不高，并且（d）缺乏强制客户输入格式正确的能力。

## 4 对于攻击和错误的健壮性

### 4.1 攻击者对模型性能的攻击

对抗性攻击的例子包括数据中毒，模型更新中毒和模型规避攻击。这些攻击可大致分为训练时攻击（中毒攻击）和推理时攻击（规避攻击）。

我们将在下面讨论推理时间攻击，因为（a）在训练阶段的攻击可以用作迈向推理时间攻击的垫脚石，并且（b）在训练过程中实施了许多针对推理时间攻击的防御措施。

#### 4.1.1 对手的目标和能力

**目标**

1. 非目标攻击或模型降级攻击，旨在降低模型的全局准确性，或“完全破坏”全局模型。

2. 目标攻击或后门攻击，旨在在维护少数示例的同时改变模型的行为 其他所有示例均具有良好的整体准确性

**能力**

![image-20200915212720562](D:\study\GitHub\Federated-Learning-Notes\Advances and Open Problems in Federated Learning\2020-09-17汇报.assets\image-20200915212720562.png)

#### 4.1.2 模型更新中毒

在联邦学习环境中，这可以通过直接破坏客户端的更新或某种中间人攻击来执行。
我们假设对手（或多个对手）直接控制一定数量的客户，并且他们可以直接更改这些客户的输出，以尝试将学习的模型偏向他们的目标。

**非目标和拜占庭式攻击**

**目标模型更新攻击**，因为对手的目的仅是影响分类 在少量数据点上获得结果的同时，在保持集中学习模型的整体准确性的同时，针对非目标攻击的防御通常无法解决目标攻击。未来工作的一个有趣途径是探索零知识证明以确保 用户正在提交具有预定属性的更新。 也可以考虑基于硬件证明的解决方案。

**联合防御** 联合学习的潜在挑战包括防御共谋或检测共谋的对手，而无需直接检查节点数据。

#### 4.1.3 数据中毒攻击

对手只能通过替换标签或数据的特定特征来操纵客户数据。

当攻击者只能影响联邦学习系统边缘的数据收集过程，而不能直接破坏学习过程中的数据时，这种攻击模型可能更常见。

差异隐私可以被视为对数据中毒的强而有力的防御，它的强项是极其笼统，无论对手的目标如何，都可提供最坏的情况下的保护，而弱点在于必须限制对手并且必须将噪声添加到联邦学习过程中。

#### 4.1.4 推理时间规避攻击

对抗训练主要是针对IID数据开发的，目前尚不清楚它在non-IID环境中的表现。

### 4.2 非恶意故障模式

**客户端报告故障** 

**数据管道故障**

 **嘈杂的模型更新**

## 5 确保公平并解决偏差的根源

### 5.1公平，隐私和鲁棒性

但是，在某些方面，公平的理想似乎与FL试图为其提供保证的隐私概念相抵触。