# 联邦学习的前沿和开放问题

### 1 引言

联邦学习是一类机器学习，在中心服务器（如：服务提供商）的管理下多个客户端（如：移动设备或整个组织）共同训练一个模型，同时保持训练数据去中心化。联邦学习体现了集中数据收集和最小化的原理，并且可以减轻由传统的集中式机器学习和数据科学方法导致的许多系统隐私风险和成本。受联邦学习研究爆炸性增长的推动，本文讨论了最新进展，并提出了大量未解决的问题和挑战。从研究和应用的角度看，该领域最近都引起了极大的兴趣。本文描述了联邦学习环境的定位和挑战，突出了重要的实践限制和注意事项，然后列举了一系列有价值的研究方向。这项工作的目的是突出具有重大理论和实践意义的研究问题，并鼓励对可能在现实世界中产生重大影响的问题进行研究。

“联邦学习“一词由McMahan等人在2016年提出：“我们将我们的方法称为联邦学习，因为学习任务是由中央服务器协调，通过松散联合的参与设备（我们称为客户端）来解决的”。不平衡且non-IID（非独立同分布）的数据，分布在通信带宽有限的大量不可靠设备中进行分区，这是关键性挑战。

重要的相关工作要早于联邦学习一词的引入。许多研究社区（包括密码学，数据库和机器学习）追求的长期目标是在不暴露数据的情况下分析和学习许多所有者之间分布的数据。用密码学方法计算加密数据始于1980年代初期，Agrawal、Srikant、Vaidya等人是试图使用集中式服务器从本地数据中学习同时保留隐私的早期代表。但是从引入联邦学习一词以来，我们也意识到，没有一项工作能够直接解决联邦学习的全部挑战。因此，术语“联邦学习”为在重要的隐私分散数据引发的机器学习问题中经常出现的一系列特征，约束和挑战提供了方便的简写。

联邦学习许多问题的关键特性是它们本质上是跨学科的，要解决这些问题可能不仅需要机器学习，还需要分布式优化，密码学，安全性，差分隐私，公平性，压缩感知，系统，信息论，统计等等。许多最棘手的问题都在这些领域的交汇处，因此，我们认为合作对于持续发展至关重要。这项工作的目标之一是强调可以将这些领域的技术进行潜在地组合的方式，既带来有趣的可能性，也带来新的挑战。

自从联邦学习一词最初以关注移动和边缘设备应用程序引入以来，将联邦学习应用于其他应用程序的兴趣就大大增加了，包括一些可能只涉及少量相对可靠的客户端的应用程序，例如多个组织合作培训 一个模型。我们将这两个联合学习分别称为“跨设备”和“跨库”。 鉴于这些变化，我们提出了对联合学习的更广泛定义： 

> 联邦学习是一种机器学习设置，其中多个实体（客户端）在中央服务器或服务提供商的协调下协作解决机器学习问题。每个客户的原始数据都存储在本地，不进行交换或转移； 取而代之的是，使用及时聚合的更新来实现学习目标。

重点更新是范围狭窄的更新，其中包含针对特定学习任务所需的最少信息。 在数据最小化服务中，尽可能早地执行聚合。我们注意到，此定义将联邦学习与完全去中心化（对等）学习技术区分开来，如2.1节所述。

尽管保护隐私的数据分析已经进行了50多年的研究，但仅在过去的十年中，解决方案才得到大规模部署。跨设备联合学习和联合数据分析现已应用于消费类数字产品。

对联邦学习技术的需求不断增长，导致出现了许多工具和框架。 其中包括TensorFlow Federated，Federated AI Technology Enabler，PySyft，Leaf，PaddleFL和Clara Training Framework； 附录A中有更多详细信息。既有技术公司也有小型初创公司正在开发结合联邦学习的商业数据平台。

表1将跨设备和跨域的联合学习与跨多个域的传统单数据中心分布式学习进行了对比。这些特征建立了实际的联邦学习系统通常必须满足的许多约束，因此可以激发和引出联邦学习中的公开挑战。 它们将在以下各节中详细讨论。

这两个联邦学习变型被称为具有代表性和重要性的案例，但不同的联邦学习环境可能具有这些特征的不同组合。 在本文的其余部分中，除非另有说明，我们将考虑跨设备联邦学习环境，尽管许多问题也适用于其他联邦学习环境。 第2节专门介绍了一些其他联邦学习的变型和应用。

接下来，我们将更详细地考虑跨设备联邦学习，重点关注该技术的典型大规模部署所共有的实践方面。 Bonawitz等人提供了针对特定生产系统的更多细节，包括对特定架构选择和考虑的讨论。

